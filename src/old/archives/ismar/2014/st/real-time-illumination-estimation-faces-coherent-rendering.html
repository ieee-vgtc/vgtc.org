---
title: Real-Time Illumination Estimation from Faces for Coherent Rendering
layout: default
permalink: /archives/ismar/2014/st/real-time-illumination-estimation-faces-coherent-rendering
breadcrumb:
  - ['Home', '/']
  - ['conference archives', '/archives']
  - ['ISMAR', '/archives/ismar']
  - ['2014', '/archives/ismar/2014']
  - ['S&amp;T Papers', '/archives/ismar/2014/st/papers']
---


  

      <span property="dc:title" content="Real-Time Illumination Estimation from Faces for Coherent Rendering" class="rdf-meta element-hidden"></span>

  

  <div class="content">

    <div class="field field-name-field-authors field-type-text field-label-above"><div class="field-label">Authors:&nbsp;</div><div class="field-items"><div class="field-item even">Sebastian B. Knorr, Daniel Kurz</div></div></div><div class="field field-name-field-doi-bookmark field-type-link-field field-label-above"><div class="field-label">DOI Bookmark:&nbsp;</div><div class="field-items"><div class="field-item even"><a href="http://dx.doi.org/10.1109/ismar.2014.6948416">http://dx.doi.org/10.1109/ismar.2014.6948416</a></div></div></div><div class="field-label">Abstract:&nbsp;</div>We present a method for estimating the real-world lighting conditions within a scene in real-time. The estimation is based on the visual appearance of a human face in the real scene captured in a single image of a monocular camera. In hardware setups featuring a user-facing camera, an image of the user's face can be acquired at any time. The limited range in variations between different human faces makes it possible to analyze their appearance offline, and to apply the results to new faces. Our approach uses radiance transfer functions - learned offline from a dataset of images of faces under different known illuminations - for particular points on the human face. Based on these functions, we recover the most plausible real-world lighting conditions for measured reflections in a face, represented by a function depending on incident light angle using Spherical Harmonics. The pose of the camera relative to the face is determined by means of optical tracking, and virtual 3D content is rendered and overlaid onto the real scene with a fixed spatial relationship to the face. By applying the estimated lighting conditions to the rendering of the virtual content, the augmented scene is shaded coherently with regard to the real and virtual parts of the scene. We show with different examples under a variety of lighting conditions, that our approach provides plausible results, which considerably enhance the visual realism in real-time Augmented Reality applications.<div id="disqus_thread"><noscript><p><a href="http://vgtc.disqus.com/?url=http%3A%2F%2Fvgtc.org%2Farchives%2Fismar%2F2014%2Fst%2Freal-time-illumination-estimation-faces-coherent-rendering">View the discussion thread.</a></p></noscript></div>  </div>

