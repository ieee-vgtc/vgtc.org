<?xml version="1.0" encoding="utf-8" ?><rss version="2.0" xml:base="http://vgtc.org/taxonomy/term/555/all" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:foaf="http://xmlns.com/foaf/0.1/" xmlns:og="http://ogp.me/ns#" xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#" xmlns:sioc="http://rdfs.org/sioc/ns#" xmlns:sioct="http://rdfs.org/sioc/types#" xmlns:skos="http://www.w3.org/2004/02/skos/core#" xmlns:xsd="http://www.w3.org/2001/XMLSchema#">
  <channel>
    <title>Short Paper</title>
    <link>http://vgtc.org/taxonomy/term/555/all</link>
    <description></description>
    <language>en</language>
          <item>
    <title>Stereoscopic Rendering of Virtual Environments with Wide Field-of-Views up to 360 </title>
    <link>http://vgtc.org/archives/vr/2014/stereoscopic-rendering-virtual-environments-wide-field-views-360</link>
    <description>&lt;div class=&quot;field field-name-field-authors field-type-text field-label-inline clearfix&quot;&gt;&lt;div class=&quot;field-label&quot;&gt;Authors:&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;field-items&quot;&gt;&lt;div class=&quot;field-item even&quot;&gt;Jerome Ardouin, Anatole Lecuyer, Maud Marchal, Eric Marchand&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;p&gt;In this paper we introduce a novel approach for stereoscopic rendering of virtual environments with a wide Field-of-View (FoV) up to 360?ï¿½. Handling such a wide FoV implies the use of non-planar projections and generates specific problems such as for rasterization and clipping. We propose a novel pre-clip stage specifically adapted to geometric approaches for which problems occur with polygons spanning across the projection discontinuities. Our approach integrates seamlessly with immersive virtual reality systems.&lt;/p&gt;&lt;div class=&quot;field field-name-field-doi-bookmark field-type-link-field field-label-inline clearfix&quot;&gt;&lt;div class=&quot;field-label&quot;&gt;DOI Bookmark:&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;field-items&quot;&gt;&lt;div class=&quot;field-item even&quot;&gt;&lt;a href=&quot;http://dx.doi.org/10.1109/VR.2014.6802042&quot;&gt;http://dx.doi.org/10.1109/VR.2014.6802042&lt;/a&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;field field-name-taxonomy-vocabulary-4 field-type-taxonomy-term-reference field-label-above&quot;&gt;&lt;div class=&quot;field-label&quot;&gt;Archives:&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;field-items&quot;&gt;&lt;div class=&quot;field-item even&quot;&gt;&lt;a href=&quot;/archives/vr/2014/short-paper&quot; typeof=&quot;skos:Concept&quot; property=&quot;rdfs:label skos:prefLabel&quot; datatype=&quot;&quot;&gt;Short Paper&lt;/a&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;field field-name-field-weight field-type-number-integer field-label-above&quot;&gt;&lt;div class=&quot;field-label&quot;&gt;Weight:&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;field-items&quot;&gt;&lt;div class=&quot;field-item even&quot;&gt;21&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description>
     <pubDate>Thu, 23 Apr 2015 03:21:00 +0000</pubDate>
 <dc:creator>admin</dc:creator>
 <guid isPermaLink="false">1148 at http://vgtc.org</guid>
  </item>
  <item>
    <title>Using Relative Head and Hand-Target Features to Predict Intention in 3D Moving-Target Selection</title>
    <link>http://vgtc.org/archives/vr/2014/using-relative-head-and-hand-target-features-predict-intention-3d-moving-target</link>
    <description>&lt;div class=&quot;field field-name-field-authors field-type-text field-label-inline clearfix&quot;&gt;&lt;div class=&quot;field-label&quot;&gt;Authors:&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;field-items&quot;&gt;&lt;div class=&quot;field-item even&quot;&gt;Juan Sebastian Casallas, James H. Oliver, Jonathan W. Kelly, Frederic Merienne, Samir Garbaya&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;p&gt;Selection of moving targets is a common, yet complex task in human-computer interaction and virtual reality. Predicting user intention may be beneficial to address the challenges inherent in interaction techniques for moving-target selection. This article extends previous models by integrating relative head-target and hand-target features to predict intended moving targets. The features are calculated in a time window ending at roughly two-thirds of the total target selection time and evaluated using decision trees.&lt;/p&gt;&lt;div class=&quot;field field-name-field-doi-bookmark field-type-link-field field-label-inline clearfix&quot;&gt;&lt;div class=&quot;field-label&quot;&gt;DOI Bookmark:&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;field-items&quot;&gt;&lt;div class=&quot;field-item even&quot;&gt;&lt;a href=&quot;http://dx.doi.org/10.1109/VR.2014.6802050&quot;&gt;http://dx.doi.org/10.1109/VR.2014.6802050&lt;/a&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;field field-name-field-running-time field-type-text field-label-above&quot;&gt;&lt;div class=&quot;field-label&quot;&gt;Running Time:&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;field-items&quot;&gt;&lt;div class=&quot;field-item even&quot;&gt;00:14:55&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;field field-name-taxonomy-vocabulary-4 field-type-taxonomy-term-reference field-label-above&quot;&gt;&lt;div class=&quot;field-label&quot;&gt;Archives:&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;field-items&quot;&gt;&lt;div class=&quot;field-item even&quot;&gt;&lt;a href=&quot;/archives/vr/2014/short-paper&quot; typeof=&quot;skos:Concept&quot; property=&quot;rdfs:label skos:prefLabel&quot; datatype=&quot;&quot;&gt;Short Paper&lt;/a&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;field field-name-field-weight field-type-number-integer field-label-above&quot;&gt;&lt;div class=&quot;field-label&quot;&gt;Weight:&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;field-items&quot;&gt;&lt;div class=&quot;field-item even&quot;&gt;29&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description>
     <pubDate>Thu, 23 Apr 2015 03:21:00 +0000</pubDate>
 <dc:creator>admin</dc:creator>
 <guid isPermaLink="false">1156 at http://vgtc.org</guid>
  </item>
  <item>
    <title>The Mind-Mirror: See Your Brain in Action in Your Head Using EEG and Augmented Reality</title>
    <link>http://vgtc.org/archives/vr/2014/mind-mirror-see-your-brain-action-your-head-using-eeg-and-augmented-reality</link>
    <description>&lt;div class=&quot;field field-name-field-authors field-type-text field-label-inline clearfix&quot;&gt;&lt;div class=&quot;field-label&quot;&gt;Authors:&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;field-items&quot;&gt;&lt;div class=&quot;field-item even&quot;&gt;Jonathan Mercier-Ganady, Fabien Lotte, Emilie Loup-Escande, Maud Marchal, Anatole Lecuyer&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;p&gt;In this paper we introduce a novel augmented reality paradigm called &quot;Mind-Mirror&quot; which enables the experience of seeing &quot;through your own head&quot;, visualizing your brain &quot;in action and in situ&quot;. A virtual brain is displayed on a screen behind a semi-transparent mirror, following head movements using an optical face-tracking system. Brain activity is extracted and processed in real-time with the help of an electroencephalography cap (EEG) worn by the user.&lt;/p&gt;&lt;div class=&quot;field field-name-field-doi-bookmark field-type-link-field field-label-inline clearfix&quot;&gt;&lt;div class=&quot;field-label&quot;&gt;DOI Bookmark:&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;field-items&quot;&gt;&lt;div class=&quot;field-item even&quot;&gt;&lt;a href=&quot;http://dx.doi.org/10.1109/VR.2014.6802047&quot;&gt;http://dx.doi.org/10.1109/VR.2014.6802047&lt;/a&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;field field-name-field-running-time field-type-text field-label-above&quot;&gt;&lt;div class=&quot;field-label&quot;&gt;Running Time:&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;field-items&quot;&gt;&lt;div class=&quot;field-item even&quot;&gt;00:11:46&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;field field-name-taxonomy-vocabulary-4 field-type-taxonomy-term-reference field-label-above&quot;&gt;&lt;div class=&quot;field-label&quot;&gt;Archives:&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;field-items&quot;&gt;&lt;div class=&quot;field-item even&quot;&gt;&lt;a href=&quot;/archives/vr/2014/short-paper&quot; typeof=&quot;skos:Concept&quot; property=&quot;rdfs:label skos:prefLabel&quot; datatype=&quot;&quot;&gt;Short Paper&lt;/a&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;field field-name-field-weight field-type-number-integer field-label-above&quot;&gt;&lt;div class=&quot;field-label&quot;&gt;Weight:&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;field-items&quot;&gt;&lt;div class=&quot;field-item even&quot;&gt;26&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description>
     <pubDate>Thu, 23 Apr 2015 03:21:00 +0000</pubDate>
 <dc:creator>admin</dc:creator>
 <guid isPermaLink="false">1153 at http://vgtc.org</guid>
  </item>
  <item>
    <title>Efficient and Robust Radiance Transfer for Probeless Photorealistic Augmented Reality</title>
    <link>http://vgtc.org/archives/vr/2014/efficient-and-robust-radiance-transfer-probeless-photorealistic-augmented-reality</link>
    <description>&lt;div class=&quot;field field-name-field-authors field-type-text field-label-inline clearfix&quot;&gt;&lt;div class=&quot;field-label&quot;&gt;Authors:&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;field-items&quot;&gt;&lt;div class=&quot;field-item even&quot;&gt;Lukas Gruber, Tobias Langlotz, Pradeep Sen, Tobias Hollerer, Dieter Schmalstieg&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;p&gt;In this work, we present an improved radiance transfer sampling approach for probeless photometric registration, which combines adaptive sampling in image and visibility space with robust caching of radiance transfer  to yield real time framerates for photorealistic AR scenes with dynamically changing scene geometry and environment lighting.&lt;/p&gt;
&lt;div class=&quot;field field-name-field-doi-bookmark field-type-link-field field-label-inline clearfix&quot;&gt;&lt;div class=&quot;field-label&quot;&gt;DOI Bookmark:&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;field-items&quot;&gt;&lt;div class=&quot;field-item even&quot;&gt;&lt;a href=&quot;http://dx.doi.org/10.1109/VR.2014.6802044&quot;&gt;http://dx.doi.org/10.1109/VR.2014.6802044&lt;/a&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;field field-name-field-running-time field-type-text field-label-above&quot;&gt;&lt;div class=&quot;field-label&quot;&gt;Running Time:&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;field-items&quot;&gt;&lt;div class=&quot;field-item even&quot;&gt;00:14:00&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;field field-name-taxonomy-vocabulary-4 field-type-taxonomy-term-reference field-label-above&quot;&gt;&lt;div class=&quot;field-label&quot;&gt;Archives:&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;field-items&quot;&gt;&lt;div class=&quot;field-item even&quot;&gt;&lt;a href=&quot;/archives/vr/2014/short-paper&quot; typeof=&quot;skos:Concept&quot; property=&quot;rdfs:label skos:prefLabel&quot; datatype=&quot;&quot;&gt;Short Paper&lt;/a&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;field field-name-field-weight field-type-number-integer field-label-above&quot;&gt;&lt;div class=&quot;field-label&quot;&gt;Weight:&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;field-items&quot;&gt;&lt;div class=&quot;field-item even&quot;&gt;23&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description>
     <pubDate>Thu, 23 Apr 2015 03:21:00 +0000</pubDate>
 <dc:creator>admin</dc:creator>
 <guid isPermaLink="false">1150 at http://vgtc.org</guid>
  </item>
  <item>
    <title>Transitional Augmented Reality Navigation for Live Captured Scenes</title>
    <link>http://vgtc.org/archives/vr/2014/transitional-augmented-reality-navigation-live-captured-scenes</link>
    <description>&lt;div class=&quot;field field-name-field-authors field-type-text field-label-inline clearfix&quot;&gt;&lt;div class=&quot;field-label&quot;&gt;Authors:&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;field-items&quot;&gt;&lt;div class=&quot;field-item even&quot;&gt;Markus Tatzgern, Raphael Grasset, Denis Kalkofen, Dieter Schmalstieg&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;p&gt;New capturing techniques based on dense Simultaneous Localization and Mapping (SLAM) not only allow users to capture real world scenes at run-time, but also enables them to capture changes of the world. However, instead of using previously recorded and prepared scenes, users must interact with an unprepared environment. We present a set of new interaction techniques that support users in handling captured real world environments. The techniques present virtual viewpoints of the scene based on a scene analysis and provide natural transitions between the AR view and virtual viewpoints.&lt;/p&gt;&lt;div class=&quot;field field-name-field-doi-bookmark field-type-link-field field-label-inline clearfix&quot;&gt;&lt;div class=&quot;field-label&quot;&gt;DOI Bookmark:&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;field-items&quot;&gt;&lt;div class=&quot;field-item even&quot;&gt;&lt;a href=&quot;http://dx.doi.org/10.1109/VR.2014.6802045&quot;&gt;http://dx.doi.org/10.1109/VR.2014.6802045&lt;/a&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;field field-name-taxonomy-vocabulary-4 field-type-taxonomy-term-reference field-label-above&quot;&gt;&lt;div class=&quot;field-label&quot;&gt;Archives:&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;field-items&quot;&gt;&lt;div class=&quot;field-item even&quot;&gt;&lt;a href=&quot;/archives/vr/2014/short-paper&quot; typeof=&quot;skos:Concept&quot; property=&quot;rdfs:label skos:prefLabel&quot; datatype=&quot;&quot;&gt;Short Paper&lt;/a&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;field field-name-field-weight field-type-number-integer field-label-above&quot;&gt;&lt;div class=&quot;field-label&quot;&gt;Weight:&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;field-items&quot;&gt;&lt;div class=&quot;field-item even&quot;&gt;24&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description>
     <pubDate>Thu, 23 Apr 2015 03:13:09 +0000</pubDate>
 <dc:creator>admin</dc:creator>
 <guid isPermaLink="false">1151 at http://vgtc.org</guid>
  </item>
  <item>
    <title>Hedgehog Labeling: View Management Techniques for External Labels in 3D Space</title>
    <link>http://vgtc.org/archives/vr/2014/hedgehog-labeling-view-management-techniques-external-labels-3d-space</link>
    <description>&lt;div class=&quot;field field-name-field-authors field-type-text field-label-inline clearfix&quot;&gt;&lt;div class=&quot;field-label&quot;&gt;Authors:&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;field-items&quot;&gt;&lt;div class=&quot;field-item even&quot;&gt;Markus Tatzgern, Denis Kalkofen, Raphael Grasset, Dieter Schmalstieg&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;p&gt;Annotations of 3D objects are commonly controlled using view management techniques. State-of-the-art view management operates in 2D image space. These approaches suffer from constant label motion, because the 2D view of a 3D scene changes over time. We propose managing the placement of external labels in 3D object space instead, which achieves temporal coherence during viewpoint changes.&lt;/p&gt;&lt;div class=&quot;field field-name-field-doi-bookmark field-type-link-field field-label-inline clearfix&quot;&gt;&lt;div class=&quot;field-label&quot;&gt;DOI Bookmark:&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;field-items&quot;&gt;&lt;div class=&quot;field-item even&quot;&gt;&lt;a href=&quot;http://dx.doi.org/10.1109/VR.2014.6802046&quot;&gt;http://dx.doi.org/10.1109/VR.2014.6802046&lt;/a&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;field field-name-taxonomy-vocabulary-4 field-type-taxonomy-term-reference field-label-above&quot;&gt;&lt;div class=&quot;field-label&quot;&gt;Archives:&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;field-items&quot;&gt;&lt;div class=&quot;field-item even&quot;&gt;&lt;a href=&quot;/archives/vr/2014/short-paper&quot; typeof=&quot;skos:Concept&quot; property=&quot;rdfs:label skos:prefLabel&quot; datatype=&quot;&quot;&gt;Short Paper&lt;/a&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;field field-name-field-weight field-type-number-integer field-label-above&quot;&gt;&lt;div class=&quot;field-label&quot;&gt;Weight:&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;field-items&quot;&gt;&lt;div class=&quot;field-item even&quot;&gt;25&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description>
     <pubDate>Thu, 23 Apr 2015 03:13:09 +0000</pubDate>
 <dc:creator>admin</dc:creator>
 <guid isPermaLink="false">1152 at http://vgtc.org</guid>
  </item>
  <item>
    <title>Temporally Enhanced 3D Capture of Room-sized Dynamic Scenes with Commodity Depth Cameras</title>
    <link>http://vgtc.org/archives/vr/2014/temporally-enhanced-3d-capture-room-sized-dynamic-scenes-commodity-depth-cameras</link>
    <description>&lt;div class=&quot;field field-name-field-authors field-type-text field-label-inline clearfix&quot;&gt;&lt;div class=&quot;field-label&quot;&gt;Authors:&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;field-items&quot;&gt;&lt;div class=&quot;field-item even&quot;&gt;Mingsong Dou, Henry Fuchs&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;p&gt;In this paper, we introduce a system to capture the enhanced 3D structure of a room-sized dynamic scene with commodity depth cameras such as Microsoft Kinects. Our system incorporates temporal information to achieve a noise-free and complete 3D capture of the entire room. More specifically, we pre-scan the static parts of the room offline, and track their movements online. For the dynamic objects, we perform non-rigid alignment between frames and accu&lt;/p&gt;
&lt;div class=&quot;field field-name-field-doi-bookmark field-type-link-field field-label-inline clearfix&quot;&gt;&lt;div class=&quot;field-label&quot;&gt;DOI Bookmark:&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;field-items&quot;&gt;&lt;div class=&quot;field-item even&quot;&gt;&lt;a href=&quot;http://dx.doi.org/10.1109/VR.2014.6802048&quot;&gt;http://dx.doi.org/10.1109/VR.2014.6802048&lt;/a&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;field field-name-taxonomy-vocabulary-4 field-type-taxonomy-term-reference field-label-above&quot;&gt;&lt;div class=&quot;field-label&quot;&gt;Archives:&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;field-items&quot;&gt;&lt;div class=&quot;field-item even&quot;&gt;&lt;a href=&quot;/archives/vr/2014/short-paper&quot; typeof=&quot;skos:Concept&quot; property=&quot;rdfs:label skos:prefLabel&quot; datatype=&quot;&quot;&gt;Short Paper&lt;/a&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;field field-name-field-weight field-type-number-integer field-label-above&quot;&gt;&lt;div class=&quot;field-label&quot;&gt;Weight:&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;field-items&quot;&gt;&lt;div class=&quot;field-item even&quot;&gt;27&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description>
     <pubDate>Thu, 23 Apr 2015 03:13:09 +0000</pubDate>
 <dc:creator>admin</dc:creator>
 <guid isPermaLink="false">1154 at http://vgtc.org</guid>
  </item>
  <item>
    <title>Reminiscence Therapy using Image-Based Rendering in VR</title>
    <link>http://vgtc.org/archives/vr/2014/reminiscence-therapy-using-image-based-rendering-vr</link>
    <description>&lt;div class=&quot;field field-name-field-authors field-type-text field-label-inline clearfix&quot;&gt;&lt;div class=&quot;field-label&quot;&gt;Authors:&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;field-items&quot;&gt;&lt;div class=&quot;field-item even&quot;&gt;Emmanuelle Chapoulie, Rachid Guerchouche, Pierre-David Petit, Gaurav Chaurasia, Philippe Robert, George Drettakis&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;p&gt;A novel VR solution for Reminiscence Therapy (RT), developed jointly by a group of memory clinicians and computer scientists, is presented. The solution consists in an immersive VR system designed for RT, which allows easy presentation of familiar environments. The system supports highly-realistic Image-based rendering in an immersive setting.  A study with healthy elderly participants is performed to test if the VR system can help with the generation of autobiographical memories.&lt;/p&gt;&lt;div class=&quot;field field-name-field-doi-bookmark field-type-link-field field-label-inline clearfix&quot;&gt;&lt;div class=&quot;field-label&quot;&gt;DOI Bookmark:&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;field-items&quot;&gt;&lt;div class=&quot;field-item even&quot;&gt;&lt;a href=&quot;http://dx.doi.org/10.1109/VR.2014.6802049&quot;&gt;http://dx.doi.org/10.1109/VR.2014.6802049&lt;/a&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;field field-name-taxonomy-vocabulary-4 field-type-taxonomy-term-reference field-label-above&quot;&gt;&lt;div class=&quot;field-label&quot;&gt;Archives:&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;field-items&quot;&gt;&lt;div class=&quot;field-item even&quot;&gt;&lt;a href=&quot;/archives/vr/2014/short-paper&quot; typeof=&quot;skos:Concept&quot; property=&quot;rdfs:label skos:prefLabel&quot; datatype=&quot;&quot;&gt;Short Paper&lt;/a&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;field field-name-field-weight field-type-number-integer field-label-above&quot;&gt;&lt;div class=&quot;field-label&quot;&gt;Weight:&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;field-items&quot;&gt;&lt;div class=&quot;field-item even&quot;&gt;28&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description>
     <pubDate>Thu, 23 Apr 2015 03:13:09 +0000</pubDate>
 <dc:creator>admin</dc:creator>
 <guid isPermaLink="false">1155 at http://vgtc.org</guid>
  </item>
  <item>
    <title>Omegalib: a Multi-View Application Framework for Hybrid Reality Display Environments</title>
    <link>http://vgtc.org/archives/vr/2014/omegalib-multi-view-application-framework-hybrid-reality-display-environments</link>
    <description>&lt;div class=&quot;field field-name-field-authors field-type-text field-label-inline clearfix&quot;&gt;&lt;div class=&quot;field-label&quot;&gt;Authors:&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;field-items&quot;&gt;&lt;div class=&quot;field-item even&quot;&gt;Alessandro Febretti, Arthur Nishimoto, Victor Mateevitsi, Luc Renambot, Andrew Johnson, Jason Leigh&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;p&gt;In the domain of large-scale visualization instruments, Hybrid Reality Environments (HREs) are a recent innovation that combines immersive environments, with the best-in-class capabilities of Ultra-high-resolution display walls. In this paper we present Omegalib, a software framework that facilitates application development on HREs. Omegalib is supports dynamic reconfigurability of the display environment: areas of the display can be interactively allocated to 2D or 3D workspaces as needed.&lt;/p&gt;&lt;div class=&quot;field field-name-field-doi-bookmark field-type-link-field field-label-inline clearfix&quot;&gt;&lt;div class=&quot;field-label&quot;&gt;DOI Bookmark:&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;field-items&quot;&gt;&lt;div class=&quot;field-item even&quot;&gt;&lt;a href=&quot;http://dx.doi.org/10.1109/VR.2014.6802043&quot;&gt;http://dx.doi.org/10.1109/VR.2014.6802043&lt;/a&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;field field-name-field-running-time field-type-text field-label-above&quot;&gt;&lt;div class=&quot;field-label&quot;&gt;Running Time:&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;field-items&quot;&gt;&lt;div class=&quot;field-item even&quot;&gt;00:12:44&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;field field-name-taxonomy-vocabulary-4 field-type-taxonomy-term-reference field-label-above&quot;&gt;&lt;div class=&quot;field-label&quot;&gt;Archives:&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;field-items&quot;&gt;&lt;div class=&quot;field-item even&quot;&gt;&lt;a href=&quot;/archives/vr/2014/short-paper&quot; typeof=&quot;skos:Concept&quot; property=&quot;rdfs:label skos:prefLabel&quot; datatype=&quot;&quot;&gt;Short Paper&lt;/a&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;field field-name-field-weight field-type-number-integer field-label-above&quot;&gt;&lt;div class=&quot;field-label&quot;&gt;Weight:&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;field-items&quot;&gt;&lt;div class=&quot;field-item even&quot;&gt;22&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description>
     <pubDate>Thu, 23 Apr 2015 03:13:09 +0000</pubDate>
 <dc:creator>admin</dc:creator>
 <guid isPermaLink="false">1149 at http://vgtc.org</guid>
  </item>
  <item>
    <title>The Effectiveness of an AR-based Context-Aware Assembly Support System in Object Assembly</title>
    <link>http://vgtc.org/archives/vr/2014/effectiveness-ar-based-context-aware-assembly-support-system-object-assembly</link>
    <description>&lt;div class=&quot;field field-name-field-authors field-type-text field-label-inline clearfix&quot;&gt;&lt;div class=&quot;field-label&quot;&gt;Authors:&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;field-items&quot;&gt;&lt;div class=&quot;field-item even&quot;&gt;Bui Minh Khuong, Kiyoshi Kiyokawa, Andrew Miller, Joseph J. LaViola Jr., Tomohiro Mashita, Haruo Takemura&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;p&gt;This study evaluates the effectiveness of an AR-based context-aware assembly support system with AR visualization modes proposed in object assembly. Our test-bed system displays guidance information and error detection information corresponding to the recognized assembly status in the context of building block (LEGO) assembly. We proposed two AR visualization modes and conducted an evaluation to comparatively evaluate these AR visualization modes as well as determine the effectiveness of context-aware error detection.&lt;/p&gt;&lt;div class=&quot;field field-name-field-doi-bookmark field-type-link-field field-label-inline clearfix&quot;&gt;&lt;div class=&quot;field-label&quot;&gt;DOI Bookmark:&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;field-items&quot;&gt;&lt;div class=&quot;field-item even&quot;&gt;&lt;a href=&quot;http://dx.doi.org/10.1109/VR.2014.6802051&quot;&gt;http://dx.doi.org/10.1109/VR.2014.6802051&lt;/a&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;field field-name-taxonomy-vocabulary-4 field-type-taxonomy-term-reference field-label-above&quot;&gt;&lt;div class=&quot;field-label&quot;&gt;Archives:&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;field-items&quot;&gt;&lt;div class=&quot;field-item even&quot;&gt;&lt;a href=&quot;/archives/vr/2014/short-paper&quot; typeof=&quot;skos:Concept&quot; property=&quot;rdfs:label skos:prefLabel&quot; datatype=&quot;&quot;&gt;Short Paper&lt;/a&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;field field-name-field-weight field-type-number-integer field-label-above&quot;&gt;&lt;div class=&quot;field-label&quot;&gt;Weight:&amp;nbsp;&lt;/div&gt;&lt;div class=&quot;field-items&quot;&gt;&lt;div class=&quot;field-item even&quot;&gt;30&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description>
     <pubDate>Thu, 23 Apr 2015 03:13:09 +0000</pubDate>
 <dc:creator>admin</dc:creator>
 <guid isPermaLink="false">1157 at http://vgtc.org</guid>
  </item>
  </channel>
</rss>
